{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7412621c-40fe-4058-958f-d0bc05528e7d",
   "metadata": {},
   "source": [
    "# Further Pre-Training Spritual-BERT Model\n",
    "\n",
    "This code fine-tunes a pre-trained BERT model (`bert-base-uncased`) on a dataset for spiritual care language modeling. Here's a detailed explanation:\n",
    "\n",
    "1. **Importing Libraries**: The code uses PyTorch for deep learning and the Hugging Face `transformers` library for pre-trained models and tokenizers. Pandas is used for data manipulation.\n",
    "\n",
    "2. **Loading and Sampling the Dataset**: The dataset `1 million sentense note.csv` is loaded, and a random sample of 1 million entries is taken from the `SentText` column. The text data is stored in a list named `texts`.\n",
    "\n",
    "3. **Preparing the BERT Model**: A tokenizer and masked language model (`BertForMaskedLM`) are loaded using the pre-trained model `bert-base-uncased`. The tokenizer processes text into numerical formats suitable for the model.\n",
    "\n",
    "4. **Tokenizing Text Data**: The text is tokenized with padding and truncation to a maximum length of 512 tokens. The output is a PyTorch tensor for efficient processing.\n",
    "\n",
    "5. **Creating a Custom Dataset**: A PyTorch `Dataset` is defined to return `input_ids`, `attention_mask`, and labels for each text. Labels are a copy of the input IDs for MLM.\n",
    "\n",
    "6. **Setting Up a Data Collator**: A `DataCollatorForLanguageModeling` is used to apply dynamic masking during training, with a 15% probability of masking tokens.\n",
    "\n",
    "7. **Configuring Training Arguments**: Training configurations include:\n",
    "   - Batch size: 4\n",
    "   - Epochs: 3\n",
    "   - Learning rate: 0.0001\n",
    "   - Checkpoints: Saved every 10,000 steps, with a maximum of 2 retained.\n",
    "   - Logging: Metrics logged every 100 steps.\n",
    "\n",
    "8. **Training the Model**: The Hugging Face `Trainer` class is used to streamline training. It combines the model, training arguments, dataset, and data collator. The training process begins with `trainer.train()`.\n",
    "\n",
    "9. **Saving the Fine-Tuned Model**: The trained model and tokenizer are saved to a directory named `Spritual_BERT`, making them available for later use.\n",
    "\n",
    "This workflow prepares a BERT model fine-tuned for domain-specific language modeling, enabling it to better understand and process text related to spiritual care.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ec3721-8af3-42e9-ba93-ad8d068cee3b",
   "metadata": {},
   "source": [
    "### Importing Libraries with Comments\n",
    "```python\n",
    "# Import PyTorch, a deep learning framework\n",
    "# Import the neural network module from PyTorch\n",
    "# Import specific components from the Hugging Face Transformers library\n",
    "# Import the random module for generating random numbers\n",
    "# Import pandas, a library for data manipulation and analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94c34c27-3959-480e-9267-45afb8281822",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertModel, logging\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d86b4ff-c260-4092-b15a-54163028c36c",
   "metadata": {},
   "source": [
    "# Explanation of the Code\n",
    "\n",
    "- The code reads a CSV file named `1 million sentense note.csv` into a pandas DataFrame using `pd.read_csv()`, enabling structured data manipulation and analysis.\n",
    "- A random sample of 1 million rows is selected from the DataFrame using `df.sample(n=1000000, random_state=1)`. This step reduces the size of the dataset for computational efficiency, and the `random_state=1` ensures the sampling process is reproducible.\n",
    "- The `SentText` column, which contains the text data, is extracted from the sampled DataFrame and converted into a Python list named `texts` using the `tolist()` method. This list format is suitable for further processing such as tokenization or analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40a3f2f9-c771-4c33-a6f2-f61d8889a66c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(\"1 million sentense note.csv\")\n",
    "# get sentence text\n",
    "df = df.sample(n=1000000, random_state=1)\n",
    "texts = df['SentText'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ffce59-db8e-4748-bf2e-c7afa81624b5",
   "metadata": {},
   "source": [
    "- **Import Libraries**: The code uses Hugging Face's `transformers` library for NLP tasks and PyTorch's `Dataset` class for creating custom datasets. These libraries enable efficient tokenization, model training, and data handling.\n",
    "\n",
    "- **Load Pre-trained Model and Tokenizer**:\n",
    "  - The BERT tokenizer and model (`bert`) are loaded using `BertTokenizer` and `BertForMaskedLM`. These are pre-trained on a general language corpus and fine-tuned for masked language modeling (MLM).\n",
    "\n",
    "- **Tokenize Text Data**:\n",
    "  - The text data (`texts`) is tokenized using the BERT tokenizer with padding, truncation, and conversion to PyTorch tensors. The `max_length=512` ensures all inputs fit within the model's constraints.\n",
    "\n",
    "- **Custom PyTorch Dataset**:\n",
    "  - A `TextDataset` class is defined to provide tokenized input IDs, attention masks, and labels for each data entry.\n",
    "  - The `__len__` method returns the number of samples, and the `__getitem__` method fetches the data for a given index.\n",
    "\n",
    "- **Data Collation**:\n",
    "  - The `DataCollatorForLanguageModeling` dynamically masks tokens during training with a probability of `0.15`, enhancing the MLM training process.\n",
    "\n",
    "- **Define Training Arguments**:\n",
    "  - The training configuration includes:\n",
    "    - Output directory: `./bio_clinical_bert_lm`.\n",
    "    - Number of epochs: 3.\n",
    "    - Batch size: 4.\n",
    "    - Learning rate: 0.0001.\n",
    "    - Checkpointing: Save every 10,000 steps, limiting to 2 checkpoints.\n",
    "    - Logging: Record metrics every 100 steps.\n",
    "    - Warm-up steps: 500.\n",
    "\n",
    "- **Create a Trainer**:\n",
    "  - The `Trainer` class simplifies model training by integrating the model, data, and training arguments.\n",
    "\n",
    "- **Start Training**:\n",
    "  - The model is trained using `trainer.train()` on the prepared dataset.\n",
    "\n",
    "- **Save the Fine-Tuned Model**:\n",
    "  - The fine-tuned model and tokenizer are saved in the `Spritual_BERT` directory for future use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe48fc4-f0b4-4281-ad08-83e23debd0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Load the Bio_ClinicalBERT tokenizer and model\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize and preprocess your text data\n",
    "tokenized_texts = tokenizer(\n",
    "    texts,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=512,  # You can adjust the max length as needed\n",
    ")\n",
    "\n",
    "# Create a PyTorch dataset for training\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokenized_texts, tokenizer):\n",
    "        self.tokenized_texts = tokenized_texts\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_texts[\"input_ids\"])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.tokenized_texts[\"input_ids\"][idx],\n",
    "            \"attention_mask\": self.tokenized_texts[\"attention_mask\"][idx],\n",
    "            \"labels\": self.tokenized_texts[\"input_ids\"][idx].clone(),\n",
    "        }\n",
    "\n",
    "dataset = TextDataset(tokenized_texts, tokenizer)\n",
    "\n",
    "# Set up data collation\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm_probability=0.15,  # You can adjust this probability\n",
    ")\n",
    "\n",
    "# Define training arguments without evaluation\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bio_clinical_bert_lm\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,  # Adjust the number of training epochs\n",
    "    per_device_train_batch_size=4,  # Adjust batch size as needed\n",
    "    save_steps=10_000,  # Save model checkpoint every n steps\n",
    "    save_total_limit=2,  # Limit the number of saved checkpoints\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,  # Log every n steps\n",
    "    learning_rate=1e-4,  # Adjust learning rate as needed\n",
    "    warmup_steps=500,  # Adjust warmup steps as needed\n",
    ")\n",
    "\n",
    "# Create a Trainer and start training\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Save the finetuned model\n",
    "model.save_pretrained(\"Spritual_BERT\")\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(\"Spritual_BERT\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IDAS Python 3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
